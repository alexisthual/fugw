
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/00_basics/plot_2_2_coarse_to_fine.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_00_basics_plot_2_2_coarse_to_fine.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_00_basics_plot_2_2_coarse_to_fine.py:


Transport distributions using sparse solvers
============================================

In this example, we sample 2 toy distributions and compute
a sparse fugw alignment between them.
Sparse alignments are typically used when both aligned distributions
have more than 10k points.

.. GENERATED FROM PYTHON SOURCE LINES 11-21

.. code-block:: default


    import matplotlib.pyplot as plt
    import torch

    from fugw.mappings import FUGW, FUGWSparse
    from fugw.scripts import coarse_to_fine
    from fugw.utils import init_mock_distribution
    from matplotlib.collections import LineCollection
    from scipy.sparse import coo_matrix








.. GENERATED FROM PYTHON SOURCE LINES 23-32

.. code-block:: default

    torch.manual_seed(13)

    n_points_source = 300
    n_samples_source = 100
    n_points_target = 300
    n_samples_target = 100
    n_features_train = 2
    n_features_test = 2








.. GENERATED FROM PYTHON SOURCE LINES 33-34

Let us generate random training data for the source and target distributions

.. GENERATED FROM PYTHON SOURCE LINES 34-41

.. code-block:: default

    _, source_features_train, _, source_embeddings = init_mock_distribution(
        n_features_train, n_points_source
    )
    _, target_features_train, _, target_embeddings = init_mock_distribution(
        n_features_train, n_points_target
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/local/lib/python3.8/site-packages/torch/distributions/wishart.py:250: UserWarning: Singular sample detected.
      warnings.warn("Singular sample detected.")




.. GENERATED FROM PYTHON SOURCE LINES 42-43

We can visualize the generated features:

.. GENERATED FROM PYTHON SOURCE LINES 43-52

.. code-block:: default

    fig = plt.figure(figsize=(4, 4))
    ax = fig.add_subplot()
    ax.set_title("Source and target features")
    ax.set_aspect("equal", "datalim")
    ax.scatter(source_features_train[0], source_features_train[1], label="Source")
    ax.scatter(target_features_train[0], target_features_train[1], label="Target")
    ax.legend()
    plt.show()




.. image-sg:: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_001.png
   :alt: Source and target features
   :srcset: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 53-55

Do not forget to normalize features and embeddings
before fitting the models.

.. GENERATED FROM PYTHON SOURCE LINES 55-70

.. code-block:: default


    source_features_train_normalized = source_features_train / torch.linalg.norm(
        source_features_train, dim=1
    ).reshape(-1, 1)
    target_features_train_normalized = target_features_train / torch.linalg.norm(
        target_features_train, dim=1
    ).reshape(-1, 1)

    source_embeddings_normalized, source_d_max = coarse_to_fine.random_normalizing(
        source_embeddings
    )
    target_embeddings_normalized, target_d_max = coarse_to_fine.random_normalizing(
        target_embeddings
    )








.. GENERATED FROM PYTHON SOURCE LINES 71-73

Let us define the coarse and fine-grained optimization problems to solve.
We also specify which solver to use at each of the 2 steps:

.. GENERATED FROM PYTHON SOURCE LINES 73-85

.. code-block:: default

    coarse_mapping = FUGW(alpha=0.5, eps=1e-4)
    coarse_mapping_solver = "mm"
    coarse_mapping_solver_params = {
        "tol_uot": 1e-10,
    }

    fine_mapping = FUGWSparse(alpha=0.5, eps=1e-4)
    fine_mapping_solver = "mm"
    fine_mapping_solver_params = {
        "tol_uot": 1e-10,
    }








.. GENERATED FROM PYTHON SOURCE LINES 88-95

Now, let us fit both the coarse and fine-grained mappings.
The coarse mapping is fitted on a limited number
of points from the source and target distributions.
You should carefully set the source and target ``selection_radius``
as they will greatly affect the sparsity of the computed mappings.
They should usally be set using domain knowledge related to the distributions
you are trying to align.

.. GENERATED FROM PYTHON SOURCE LINES 95-120

.. code-block:: default

    _, _ = coarse_to_fine.fit(
        # Source and target's features and embeddings
        source_features=source_features_train_normalized,
        target_features=target_features_train_normalized,
        source_geometry_embeddings=source_embeddings_normalized,
        target_geometry_embeddings=target_embeddings_normalized,
        # Parametrize step 1 (coarse alignment between source and target)
        source_sample_size=n_samples_source,
        target_sample_size=n_samples_target,
        coarse_mapping=coarse_mapping,
        coarse_mapping_solver=coarse_mapping_solver,
        coarse_mapping_solver_params=coarse_mapping_solver_params,
        # Parametrize step 2 (selection of pairs of indices present in
        # fine-grained's sparsity mask)
        coarse_pairs_selection_method="topk",
        source_selection_radius=1 / source_d_max,
        target_selection_radius=1 / target_d_max,
        # Parametrize step 3 (fine-grained alignment)
        fine_mapping=fine_mapping,
        fine_mapping_solver=fine_mapping_solver,
        fine_mapping_solver_params=fine_mapping_solver_params,
        # Misc
        verbose=True,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none



    [21:00:23] BCD step 1/10   FUGW loss:      0.03198949992656708      dense.py:360
               (base)      0.032095134258270264 (entropic)                          


    [21:00:24] BCD step 2/10   FUGW loss:      0.027364276349544525     dense.py:360
               (base)     0.027612881734967232 (entropic)                           


               BCD step 3/10   FUGW loss:      0.017961936071515083     dense.py:360
               (base)     0.01844286173582077 (entropic)                            


               BCD step 4/10   FUGW loss:      0.015194778330624104     dense.py:360
               (base)     0.015796463936567307 (entropic)                           


    [21:00:25] BCD step 5/10   FUGW loss:      0.014496020041406155     dense.py:360
               (base)     0.015153813175857067 (entropic)                           


               BCD step 6/10   FUGW loss:      0.014186171814799309     dense.py:360
               (base)     0.014875920489430428 (entropic)                           


    [21:00:26] BCD step 7/10   FUGW loss:      0.014005331322550774     dense.py:360
               (base)     0.01471529621630907 (entropic)                            


    [21:00:27] BCD step 8/10   FUGW loss:      0.013867946341633797     dense.py:360
               (base)     0.014592503197491169 (entropic)                           


    [21:00:28] BCD step 9/10   FUGW loss:      0.013740603812038898     dense.py:360
               (base)     0.014477620832622051 (entropic)                           


    [21:00:30] BCD step 10/10  FUGW loss:      0.013632165268063545     dense.py:360
               (base)     0.014380687847733498 (entropic)                           
      100% Sparsity mask ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 0:00:01 < 0:00:00
    /github/workspace/src/fugw/utils.py:50: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)
      return x.to(device, dtype).to_sparse_csr()


    [21:00:44] BCD step 1/10   FUGW loss:      0.02426101453602314     sparse.py:478
               (base)      0.024409931153059006 (entropic)                          


    [21:00:47] BCD step 2/10   FUGW loss:      0.01927129738032818     sparse.py:478
               (base)      0.019580353051424026 (entropic)                          


    [21:00:50] BCD step 3/10   FUGW loss:      0.014165128581225872    sparse.py:478
               (base)     0.014671804383397102 (entropic)                           


    [21:00:53] BCD step 4/10   FUGW loss:      0.012343265116214752    sparse.py:478
               (base)     0.01297290250658989 (entropic)                            


    [21:00:56] BCD step 5/10   FUGW loss:      0.011675005778670311    sparse.py:478
               (base)     0.012375805526971817 (entropic)                           


    [21:01:00] BCD step 6/10   FUGW loss:      0.01134925801306963     sparse.py:478
               (base)      0.012095878832042217 (entropic)                          


    [21:01:04] BCD step 7/10   FUGW loss:      0.011161427944898605    sparse.py:478
               (base)     0.011940025724470615 (entropic)                           


    [21:01:09] BCD step 8/10   FUGW loss:      0.011038612574338913    sparse.py:478
               (base)     0.011841125786304474 (entropic)                           


    [21:01:14] BCD step 9/10   FUGW loss:      0.010950836353003979    sparse.py:478
               (base)     0.011772247031331062 (entropic)                           


    [21:01:21] BCD step 10/10  FUGW loss:      0.010884434916079044    sparse.py:478
               (base)     0.011721358634531498 (entropic)                           




.. GENERATED FROM PYTHON SOURCE LINES 121-123

Both the coarse and fine-grained transport plans can be accessed
after the models have been fitted

.. GENERATED FROM PYTHON SOURCE LINES 123-129

.. code-block:: default

    print(f"Coarse transport plan's total mass: {coarse_mapping.pi.sum():.5f}")
    print(
        "Fine-grained transport plan's total mass:"
        f" {torch.sparse.sum(fine_mapping.pi):.5f}"
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Coarse transport plan's total mass: 0.99738
    Fine-grained transport plan's total mass: 0.99798




.. GENERATED FROM PYTHON SOURCE LINES 130-132

Here is the evolution of the FUGW loss during training
of the coarse mapping, with and without the entropic term:

.. GENERATED FROM PYTHON SOURCE LINES 132-146

.. code-block:: default


    fig, ax = plt.subplots(figsize=(4, 4))
    ax.set_title("Coarse mapping training loss")
    ax.set_ylabel("Loss")
    ax.set_xlabel("BCD step")
    ax.plot(coarse_mapping.loss_steps, coarse_mapping.loss, label="FUGW loss")
    ax.plot(
        coarse_mapping.loss_steps,
        coarse_mapping.loss_entropic,
        label="FUGW entropic loss",
    )
    ax.legend()
    plt.show()




.. image-sg:: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_002.png
   :alt: Coarse mapping training loss
   :srcset: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 147-148

And here is that of the fine-grained mapping:

.. GENERATED FROM PYTHON SOURCE LINES 148-162

.. code-block:: default


    fig, ax = plt.subplots(figsize=(4, 4))
    ax.set_title("Fine-grained mapping training loss")
    ax.set_ylabel("Loss")
    ax.set_xlabel("BCD step")
    ax.plot(fine_mapping.loss_steps, fine_mapping.loss, label="FUGW loss")
    ax.plot(
        fine_mapping.loss_steps,
        fine_mapping.loss_entropic,
        label="FUGW entropic loss",
    )
    ax.legend()
    plt.show()




.. image-sg:: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_003.png
   :alt: Fine-grained mapping training loss
   :srcset: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 163-168

In this example, the computed sparse transport plan is not very sparse:
it stores about 60% of what the equivalent dense transport plan
would store.
When aligning distributions with a high number of points,
we usually want to keep this number much smaller.

.. GENERATED FROM PYTHON SOURCE LINES 168-174

.. code-block:: default


    sparsity_ratio = (
        100 * fine_mapping.pi.values().shape[0] / fine_mapping.pi.shape.numel()
    )
    print(f"Ratio of non-null coefficients: {sparsity_ratio:.2f}%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Ratio of non-null coefficients: 62.82%




.. GENERATED FROM PYTHON SOURCE LINES 175-178

We can also have a look at the sparsity mask of the
fine-grained transport plan. In this particular example,
we don't expect it to show a particularly meaningful structure.

.. GENERATED FROM PYTHON SOURCE LINES 178-194

.. code-block:: default

    indices = fine_mapping.pi.indices()
    fine_mapping_as_scipy_coo = coo_matrix(
        (
            fine_mapping.pi.values(),
            (indices[0], indices[1]),
        ),
        shape=fine_mapping.pi.size(),
    )

    fig, ax = plt.subplots(figsize=(5, 5))
    ax.set_title("Sparsity mask of fine-grained mapping")
    ax.set_ylabel("Source vertices")
    ax.set_xlabel("Target vertices")
    plt.spy(fine_mapping_as_scipy_coo, precision="present", markersize=0.3)
    plt.show()




.. image-sg:: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_004.png
   :alt: Sparsity mask of fine-grained mapping
   :srcset: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 195-197

We can observe the computed mappings between source
and target points in the feature space.

.. GENERATED FROM PYTHON SOURCE LINES 197-226

.. code-block:: default

    pi = fine_mapping.pi.to_dense()
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot()
    ax.set_aspect("equal", "datalim")
    ax.set_title("Mapping\ndisplayed in feature space")

    # Draw lines between matched points
    indices = torch.cartesian_prod(
        torch.arange(n_points_source), torch.arange(n_points_target)
    )
    segments = torch.stack(
        [
            source_features_train[:, indices[:, 0]],
            target_features_train[:, indices[:, 1]],
        ]
    ).permute(2, 0, 1)
    pi_normalized = pi / pi.sum(dim=1).reshape(-1, 1)
    line_segments = LineCollection(
        segments, alpha=pi_normalized.flatten(), colors="black", lw=1, zorder=1
    )
    ax.add_collection(line_segments)

    # Draw distributions
    ax.scatter(source_features_train[0], source_features_train[1], label="Source")
    ax.scatter(target_features_train[0], target_features_train[1], label="Target")

    ax.legend()
    plt.show()




.. image-sg:: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_005.png
   :alt: Mapping displayed in feature space
   :srcset: /auto_examples/00_basics/images/sphx_glr_plot_2_2_coarse_to_fine_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 227-229

Finally, the fitted fine-grained model can transport unseen data
between source and target

.. GENERATED FROM PYTHON SOURCE LINES 229-234

.. code-block:: default

    source_features_test = torch.rand(n_features_test, n_points_source)
    target_features_test = torch.rand(n_features_test, n_points_target)
    transformed_data = fine_mapping.transform(source_features_test)
    transformed_data.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    torch.Size([2, 300])



.. GENERATED FROM PYTHON SOURCE LINES 235-236

.. code-block:: default

    assert transformed_data.shape == target_features_test.shape








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  8.364 seconds)

**Estimated memory usage:**  1519 MB


.. _sphx_glr_download_auto_examples_00_basics_plot_2_2_coarse_to_fine.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_2_2_coarse_to_fine.py <plot_2_2_coarse_to_fine.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_2_2_coarse_to_fine.ipynb <plot_2_2_coarse_to_fine.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
