{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using a validation set to monitor the convergence of the fugw loss\n\nIn this example, we use 3 fMRI feature maps for training and 2 independant\nfMRI feature maps for testing to examine the evolutions of a training and\na validation loss on 2 low-resolution brain volumes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom nilearn import datasets, image, plotting\nfrom scipy.spatial import distance_matrix\nfrom fugw.mappings import FUGW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first fetch 5 contrasts for each subject from the localizer dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_subjects = 2\n\ncontrasts = [\n    \"sentence reading vs checkerboard\",\n    \"sentence listening\",\n    \"calculation vs sentences\",\n    \"left vs right button press\",\n    \"checkerboard\",\n]\nn_training_contrasts = 3\n\nbrain_data = datasets.fetch_localizer_contrasts(\n    contrasts,\n    n_subjects=n_subjects,\n    get_anats=True,\n    get_masks=True,\n)\n\nsource_imgs_paths = brain_data[\"cmaps\"][0 : len(contrasts)]\ntarget_imgs_paths = brain_data[\"cmaps\"][len(contrasts) : 2 * len(contrasts)]\nsource_mask = brain_data[\"masks\"][0]\n\nsource_im = image.load_img(source_imgs_paths)\ntarget_im = image.load_img(target_imgs_paths)\nmask = image.load_img(source_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then downsample the images by 5 to reduce the computational cost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SCALE_FACTOR = 5\n\nresized_source_affine = source_im.affine.copy() * SCALE_FACTOR\nresized_target_affine = target_im.affine.copy() * SCALE_FACTOR\n\nsource_im_resized = image.resample_img(source_im, resized_source_affine)\ntarget_im_resized = image.resample_img(target_im, resized_target_affine)\nmask_resized = image.resample_img(mask, resized_source_affine)\n\nsource_maps = np.nan_to_num(source_im_resized.get_fdata())\ntarget_maps = np.nan_to_num(target_im_resized.get_fdata())\nsegmentation = mask_resized.get_fdata()\n\ncoordinates = np.argwhere(segmentation > 0)\n\nsource_features = source_maps[\n    coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]\n].T\ntarget_features = target_maps[\n    coordinates[:, 0], coordinates[:, 1], coordinates[:, 2]\n].T\n\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.scatter(coordinates[:, 0], coordinates[:, 1], coordinates[:, 2], marker=\"o\")\nax.view_init(10, 135)\n# make the panes transparent\nax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\nax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\nax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n# make the grid lines transparent\nax.xaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0)\nax.yaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0)\nax.zaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0)\nax.set_title(\"3D voxel coordinates\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then compute the distance matrix between voxel coordinates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_geometry = distance_matrix(coordinates, coordinates)\ntarget_geometry = source_geometry.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to avoid numerical errors when fitting the mapping, we normalize\nboth the features and the geometry.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_features_normalized = source_features / np.linalg.norm(\n    source_features, axis=1\n).reshape(-1, 1)\ntarget_features_normalized = target_features / np.linalg.norm(\n    target_features, axis=1\n).reshape(-1, 1)\nsource_geometry_normalized = source_geometry / np.max(source_geometry)\ntarget_geometry_normalized = target_geometry / np.max(target_geometry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now fit the mapping using the sinkhorn solver and 10 BCD iterations.\nWe use the first 3 feature maps for training and the last 2 for validation.\nAnatomical kernels are kept identical for both training and validation,\nas it will usually be the case in practice when aligning real fMRI data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapping = FUGW(alpha=0.5, rho=1, eps=1e-4)\n_ = mapping.fit(\n    source_features=source_features_normalized[:n_training_contrasts],\n    target_features=target_features_normalized[:n_training_contrasts],\n    source_geometry=source_geometry_normalized,\n    target_geometry=target_geometry_normalized,\n    source_features_val=source_features_normalized[n_training_contrasts:],\n    target_features_val=target_features_normalized[n_training_contrasts:],\n    solver=\"sinkhorn\",\n    solver_params={\n        \"nits_bcd\": 10,\n    },\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the evolution of losses on train and test datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots()\nax1.set_xlabel(\"BCD Step\")\nax1.set_ylabel(\"FUGW loss\", color=\"black\")\nax1.tick_params(axis=\"y\", labelcolor=\"black\")\n\nax1.plot(mapping.loss_steps, mapping.loss[\"total\"], color=\"blue\")\nax1.plot(mapping.loss_steps, mapping.loss_val[\"total\"], color=\"red\")\n\nplt.title(\"Training and validation losses\")\nplt.legend([\"Train\", \"Validation\"])\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the alignment of the second validation feature map\nand project it on the fsaverage5 surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "example_array = np.nan_to_num(source_im_resized.slicer[..., -1].get_fdata())\nexample_array /= np.max(np.abs(example_array))\nexample = image.new_img_like(source_im_resized, example_array)\nplotting.view_img_on_surf(example, threshold=\"50%\", surf_mesh=\"fsaverage5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}