{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Align low-resolution brain surfaces of 2 individuals with fMRI data\n\nIn this example, we align 2 low-resolution left hemispheres\nusing 4 fMRI feature maps (z-score contrast maps).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gdist\nimport matplotlib as mpl\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom fugw.mappings import FUGW\nfrom fugw.utils import load_mapping, save_mapping\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom nilearn import datasets, image, plotting, surface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's download 5 volumetric contrast maps per individual\nusing ``nilearn``'s API. We will use the first 4 of them\nto compute an alignment between the source and target subjects,\nand use the left-out contrast to assess the quality of our alignment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_subjects = 2\n\ncontrasts = [\n    \"sentence reading vs checkerboard\",\n    \"sentence listening\",\n    \"calculation vs sentences\",\n    \"left vs right button press\",\n    \"checkerboard\",\n]\nn_training_contrasts = 4\n\nbrain_data = datasets.fetch_localizer_contrasts(\n    contrasts,\n    n_subjects=n_subjects,\n    get_anats=True,\n)\n\nsource_imgs_paths = brain_data[\"cmaps\"][0 : len(contrasts)]\ntarget_imgs_paths = brain_data[\"cmaps\"][len(contrasts) : 2 * len(contrasts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is what the first contrast map of the source subject looks like\n(the following figure is interactive):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_index = 0\nplotting.view_img(\n    source_imgs_paths[contrast_index],\n    brain_data[\"anats\"][0],\n    title=f\"Contrast {contrast_index} (source subject)\",\n    opacity=0.5,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing feature arrays\nLet's project these 4 maps to a mesh representing the cortical surface\nand aggregate these projections to build an array of features for the\nsource and target subjects.\nFor the sake of keeping the training phase of our mapping short even on CPU,\nwe project these volumetric maps on a very low-resolution mesh\nmade of 642 vertices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fsaverage3 = datasets.fetch_surf_fsaverage(mesh=\"fsaverage3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_images_and_project_to_surface(image_paths):\n    \"\"\"Util function for loading and projecting volumetric images.\"\"\"\n    images = [image.load_img(img) for img in image_paths]\n    surface_images = [\n        np.nan_to_num(surface.vol_to_surf(img, fsaverage3.pial_left))\n        for img in images\n    ]\n\n    return np.stack(surface_images)\n\n\nsource_features = load_images_and_project_to_surface(source_imgs_paths)\ntarget_features = load_images_and_project_to_surface(target_imgs_paths)\nsource_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a figure showing the 4 projected maps for each of\nthe 2 individuals:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_surface_map(surface_map, cmap=\"coolwarm\", colorbar=True, **kwargs):\n    \"\"\"Util function for plotting surfaces.\"\"\"\n    plotting.plot_surf(\n        fsaverage3.pial_left,\n        surface_map,\n        cmap=cmap,\n        colorbar=colorbar,\n        bg_map=fsaverage3.sulc_left,\n        bg_on_data=True,\n        darkness=0.5,\n        **kwargs,\n    )\n\n\nfig = plt.figure(figsize=(3 * n_subjects, 3 * len(contrasts)))\ngrid_spec = gridspec.GridSpec(len(contrasts), n_subjects, figure=fig)\n\n# Print all feature maps\nfor i, contrast_name in enumerate(contrasts):\n    for j, features in enumerate([source_features, target_features]):\n        ax = fig.add_subplot(grid_spec[i, j], projection=\"3d\")\n        plot_surface_map(\n            features[i, :], axes=ax, vmax=10, vmin=-10, colorbar=False\n        )\n\n    # Add labels to subplots\n    if i == 0:\n        for j in range(2):\n            ax = fig.add_subplot(grid_spec[i, j])\n            ax.axis(\"off\")\n            ax.text(0.5, 1, f\"sub-0{j}\", va=\"center\", ha=\"center\")\n\n    ax = fig.add_subplot(grid_spec[i, :])\n    ax.axis(\"off\")\n    ax.text(0.5, 0, contrast_name, va=\"center\", ha=\"center\")\n\n# Add colorbar\nax = fig.add_subplot(grid_spec[2, :])\nax.axis(\"off\")\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"2%\")\nfig.add_axes(cax)\nfig.colorbar(\n    mpl.cm.ScalarMappable(\n        norm=mpl.colors.Normalize(vmin=-10, vmax=10), cmap=\"coolwarm\"\n    ),\n    cax=cax,\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing geometry arrays\nNow we compute the kernel matrix of distances between vertices\non the cortical surface. Note that in this example,\nwe are using the same mesh for the source and target individuals,\nbut this does not have to be the case in general.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_geometry_from_mesh(mesh_path):\n    \"\"\"Util function to compute matrix of geodesic distances of a mesh.\"\"\"\n    (coordinates, triangles) = surface.load_surf_mesh(mesh_path)\n    geometry = gdist.local_gdist_matrix(\n        coordinates.astype(np.float64), triangles.astype(np.int32)\n    ).toarray()\n\n    return geometry\n\n\nfsaverage3_pial_left_geometry = compute_geometry_from_mesh(\n    fsaverage3.pial_left\n)\nsource_geometry = fsaverage3_pial_left_geometry\ntarget_geometry = fsaverage3_pial_left_geometry\nsource_geometry.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each line ``vertex_index`` of the geometry matrices contains the anatomical\ndistance (here in millimeters) from ``vertex_index`` to all other vertices\nof the mesh.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vertex_index = 4\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"Geodesic distance in mm\\non the cortical surface\")\nplot_surface_map(\n    source_geometry[vertex_index, :],\n    cmap=\"magma\",\n    cbar_tick_format=\"%.2f\",\n    axes=ax,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing features and geometries\nFeatures and geometries should be normalized before we can train a mapping.\nIndeed, without this scaling, it's unclear whether the source and target\nfeatures would be comparable. Moreover, the hyper-parameter ``alpha`` would\ndepend on the scale of the respective matrices. Finally, it can empirically\nlead to having ``nan`` values in the computed transport plan.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_features_normalized = source_features / np.linalg.norm(\n    source_features, axis=1\n).reshape(-1, 1)\ntarget_features_normalized = target_features / np.linalg.norm(\n    target_features, axis=1\n).reshape(-1, 1)\nsource_geometry_normalized = source_geometry / np.max(source_geometry)\ntarget_geometry_normalized = target_geometry / np.max(target_geometry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the mapping\nLet's create our mapping. We set ``alpha=0.5`` to indicate that we are\nas interested in matching vertices with similar features as we are in\npreserving the anatomical geometries of the source and target subjects.\nWe leave ``rho`` to its default value, and finally set a value of ``eps``\nwhich is low enough for the computed transport plan to not be too\nregularized. High values of ``eps`` lead to faster computations\nand more regularized (ie blurry) plans.\nLow values of ``eps`` lead to solwer computations, but finer-grained plans.\nNote that this package is meant to be used with GPUs ; fitting mappings\non CPUs in about 100x slower.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.5\nrho = 1\neps = 1e-4\nmapping = FUGW(alpha=alpha, rho=rho, eps=eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's fit our mapping! \ud83d\ude80\n\nRemember to use the training maps only.\nMoreover, we limit the number of block-coordinate-descent\niterations to 3 in order to limit computation time for this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = mapping.fit(\n    source_features_normalized[:n_training_contrasts],\n    target_features_normalized[:n_training_contrasts],\n    source_geometry=source_geometry_normalized,\n    target_geometry=target_geometry_normalized,\n    solver=\"sinkhorn\",\n    solver_params={\n        \"nits_bcd\": 3,\n    },\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the evolution of the FUGW loss during training,\nwith and without the regularized term:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 4))\nax.set_title(\n    \"Sinkhorn mapping training loss\\n\"\n    f\"Total training time = {mapping.loss_times[-1]:.1f}s\"\n)\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"BCD step\")\nax.stackplot(\n    mapping.loss_steps,\n    [\n        (1 - alpha) * np.array(mapping.loss[\"wasserstein\"]),\n        alpha * np.array(mapping.loss[\"gromov_wasserstein\"]),\n        rho * np.array(mapping.loss[\"marginal_constraint_dim1\"]),\n        rho * np.array(mapping.loss[\"marginal_constraint_dim2\"]),\n        eps * np.array(mapping.loss[\"regularization\"]),\n    ],\n    labels=[\n        \"wasserstein\",\n        \"gromov_wasserstein\",\n        \"marginal_constraint_dim1\",\n        \"marginal_constraint_dim2\",\n        \"regularization\",\n    ],\n    alpha=0.8,\n)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we used the ``sinkhorn`` solver here because it's well known\nin the optimal transport community, but that\nthis library comes with other solvers which are, in most cases,\nmuch faster.\nLet's retrain our mapping using the ``mm`` solver, which implements\na maximize-minimization approach to approximate a solution and is\nused by default in ``fugw.mappings``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mm_mapping = FUGW(alpha=alpha, rho=rho, eps=eps)\n\n_ = mm_mapping.fit(\n    source_features_normalized[:n_training_contrasts],\n    target_features_normalized[:n_training_contrasts],\n    source_geometry=source_geometry_normalized,\n    target_geometry=target_geometry_normalized,\n    solver=\"mm\",\n    solver_params={\n        \"nits_bcd\": 5,\n        \"tol_bcd\": 1e-10,\n        \"tol_uot\": 1e-10,\n    },\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now with the ``ibpp`` solver:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ibpp_mapping = FUGW(alpha=alpha, rho=rho, eps=eps)\n\n_ = ibpp_mapping.fit(\n    source_features_normalized[:n_training_contrasts],\n    target_features_normalized[:n_training_contrasts],\n    source_geometry=source_geometry_normalized,\n    target_geometry=target_geometry_normalized,\n    solver=\"ibpp\",\n    solver_params={\n        \"nits_bcd\": 5,\n        \"tol_bcd\": 1e-10,\n        \"tol_uot\": 1e-10,\n    },\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computed mappings can easily be saved on disk and loaded back.\nNote that `fugw.mappings` overwrite functions used by `pickle`\nso that hyper-parameters and model weights are stored separately.\nThis is handy if you want to quickly load a mapping without\nits weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save mappings\nsave_mapping(mapping, \"./mapping.pkl\")\n\n# Load mappings\nmapping = load_mapping(\"./mapping.pkl\")\n# Load mappings hyper-parameters only\nmapping_without_weights = load_mapping(\"./mapping.pkl\", load_weights=False)\n\nprint(f\"With weights: pi = tensor of size {mapping.pi.shape}\")\nprint(f\"Without weights: pi = {mapping_without_weights.pi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the evolution of the FUGW loss during training,\nwithout the regularized term. Note how, in this case,\neven though ``mm`` and ``ibpp`` needed more block-coordinate-descent steps\nto converge, they were about 2 to 3 times faster to reach the same final\nFUGW training loss as ``sinkhorn``.\nYou might want to tweak solver parameters like ``nits_bcd`` and ``nits_uot``\nto get the fastest convergence rates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(4 * 2, 4))\nfig.suptitle(\"Training loss comparison\\nSinkhorn vs MM vs IBPP\")\n\nax = fig.add_subplot(121)\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"BCD step\")\nax.plot(mapping.loss_steps, mapping.loss[\"total\"], label=\"Sinkhorn FUGW loss\")\nax.plot(mm_mapping.loss_steps, mm_mapping.loss[\"total\"], label=\"MM FUGW loss\")\nax.plot(\n    ibpp_mapping.loss_steps, ibpp_mapping.loss[\"total\"], label=\"IBPP FUGW loss\"\n)\nax.legend()\n\nax = fig.add_subplot(122)\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Time (in seconds)\")\nax.plot(mapping.loss_times, mapping.loss[\"total\"], label=\"FUGW loss\")\nax.plot(mm_mapping.loss_times, mm_mapping.loss[\"total\"], label=\"MM FUGW loss\")\nax.plot(\n    ibpp_mapping.loss_times, ibpp_mapping.loss[\"total\"], label=\"IBPP FUGW loss\"\n)\nax.legend()\n\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the computed mapping\nThe computed mapping is stored in ``mapping.pi`` as a ``torch.Tensor``.\nIn this example, the transport plan is small enough that we can display\nit altogether.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pi = mapping.pi.numpy()\nfig, ax = plt.subplots(figsize=(10, 10))\nax.set_title(\"Transport plan\", fontsize=20)\nax.set_xlabel(\"target vertices\", fontsize=15)\nax.set_ylabel(\"source vertices\", fontsize=15)\nim = plt.imshow(pi, cmap=\"viridis\")\nplt.colorbar(im, ax=ax, shrink=0.8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each line ``vertex_index`` of the computed mapping can be interpreted as\na probability map describing which vertices of the target\nshould be mapped with the source vertex ``vertex_index``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probability_map = pi[vertex_index, :] / np.linalg.norm(pi[vertex_index, :])\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\n    \"Probability map of target vertices\\n\"\n    f\"being matched with source vertex {vertex_index}\"\n)\nplot_surface_map(probability_map, cmap=\"viridis\", axes=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using ``mapping.transform()``,\nwe can use the computed mapping to transport any collection of feature maps\nfrom the source anatomy onto the target anatomy.\nNote that, conversely, ``mapping.inverse_transform()`` takes feature maps\nfrom the target anatomy and transports them on the source anatomy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_index = 2\npredicted_target_features = mapping.transform(\n    source_features[contrast_index, :]\n)\npredicted_target_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(3 * 3, 3))\nfig.suptitle(\"Transporting feature maps of the training set\")\ngrid_spec = gridspec.GridSpec(1, 3, figure=fig)\n\nax = fig.add_subplot(grid_spec[0, 0], projection=\"3d\")\nax.set_title(\"Actual source features\")\nplot_surface_map(\n    source_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nax = fig.add_subplot(grid_spec[0, 1], projection=\"3d\")\nax.set_title(\"Predicted target features\")\nplot_surface_map(predicted_target_features, axes=ax, vmax=10, vmin=-10)\n\nax = fig.add_subplot(grid_spec[0, 2], projection=\"3d\")\nax.set_title(\"Actual target features\")\nplot_surface_map(\n    target_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we transported a feature map which is part of the traning set,\nwhich does not really help evaluate the quality of our model.\nInstead, we can also use the computed mapping to transport unseen data,\nwhich is how we will usually assess whether our model has captured\nuseful information or not:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_index = len(contrasts) - 1\npredicted_target_features = mapping.transform(\n    source_features[contrast_index, :]\n)\n\nfig = plt.figure(figsize=(3 * 3, 3))\nfig.suptitle(\"Transporting feature maps of the test set\")\ngrid_spec = gridspec.GridSpec(1, 3, figure=fig)\n\nax = fig.add_subplot(grid_spec[0, 0], projection=\"3d\")\nax.set_title(\"Actual source features\")\nplot_surface_map(\n    source_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nax = fig.add_subplot(grid_spec[0, 1], projection=\"3d\")\nax.set_title(\"Predicted target features\")\nplot_surface_map(predicted_target_features, axes=ax, vmax=10, vmin=-10)\n\nax = fig.add_subplot(grid_spec[0, 2], projection=\"3d\")\nax.set_title(\"Actual target features\")\nplot_surface_map(\n    target_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}