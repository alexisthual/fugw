{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Align high-resolution brain surfaces of 2 individuals with fMRI data\n\nIn this example, we show how to use this package to align 2\nhigh-resolution left hemispheres using fMRI feature maps\n(z-score contrast maps).\nNote that, since we want this example to run on CPU,\nwe stick to rather low-resolution meshes (around 10k vertices\nper hemisphere) but that this package can easily scale to resolutions\nabove 150k vertices per hemisphere. In this case, with appropriate\nhyper-parameters and solver parameters, it takes less than\n10 minutes to compute a mapping between 2 such distributions\nusing a V100 Nvidia GPU.\n\n**Before reading this tutorial, you should first go through**\n`the example aligning brain data at a low-resolution\n<sphx_glr_auto_examples_01_brain_alignment_plot_1_aligning_brain_dense.py>`,\nwhich explains the ropes of brain alignment in more detail\nthan this example.\nIn the current example, we focus more on how to use this package\nto align data using real-life resolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport time\n\nimport matplotlib as mpl\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom fugw.mappings import FUGW, FUGWSparse\nfrom fugw.scripts import coarse_to_fine, lmds\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom nilearn import datasets, image, plotting, surface\nfrom nilearn.plotting.surf_plotting import CAMERAS, LAYOUT\nfrom plotly.subplots import make_subplots\nfrom scipy.sparse import coo_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's download 5 volumetric contrast maps per individual\nusing ``nilearn``'s API. We will use the first 4 of them\nto compute an alignment between the source and target subjects,\nand use the left-out contrast to assess the quality of our alignment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\ntorch.manual_seed(0)\n\nn_subjects = 2\n\ncontrasts = [\n    \"sentence reading vs checkerboard\",\n    \"sentence listening\",\n    \"calculation vs sentences\",\n    \"left vs right button press\",\n    \"checkerboard\",\n]\nn_training_contrasts = 4\n\nbrain_data = datasets.fetch_localizer_contrasts(\n    contrasts,\n    n_subjects=n_subjects,\n    get_anats=True,\n)\n\nsource_imgs_paths = brain_data[\"cmaps\"][0 : len(contrasts)]\ntarget_imgs_paths = brain_data[\"cmaps\"][len(contrasts) : 2 * len(contrasts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing feature arrays\nLet's project these 4 maps to a mesh of the cortical surface\nand aggregate these projections to build an array of features for the\nsource and target subjects.\nFor the sake of keeping the training phase of our mapping short even on CPU,\nwe project these volumetric maps on a low-resolution mesh\nmade of 10242 vertices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fsaverage5 = datasets.fetch_surf_fsaverage(mesh=\"fsaverage5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_images_and_project_to_surface(image_paths):\n    \"\"\"Util function for loading and projecting volumetric images.\"\"\"\n    images = [image.load_img(img) for img in image_paths]\n    surface_images = [\n        np.nan_to_num(surface.vol_to_surf(img, fsaverage5.pial_left))\n        for img in images\n    ]\n\n    return np.stack(surface_images)\n\n\nsource_features = load_images_and_project_to_surface(source_imgs_paths)\ntarget_features = load_images_and_project_to_surface(target_imgs_paths)\nsource_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a figure showing the 4 projected maps for each of\nthe 2 individuals:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_surface_map(\n    surface_map, cmap=\"coolwarm\", colorbar=True, engine=\"matplotlib\", **kwargs\n):\n    \"\"\"Util function for plotting surfaces.\"\"\"\n    return plotting.plot_surf(\n        fsaverage5.pial_left,\n        surface_map,\n        cmap=cmap,\n        colorbar=colorbar,\n        bg_map=fsaverage5.sulc_left,\n        bg_on_data=True,\n        darkness=0.5,\n        engine=engine,\n        **kwargs,\n    )\n\n\nfig = plt.figure(figsize=(3 * n_subjects, 3 * len(contrasts)))\ngrid_spec = gridspec.GridSpec(len(contrasts), n_subjects, figure=fig)\n\n# Print all feature maps\nfor i, contrast_name in enumerate(contrasts):\n    for j, features in enumerate([source_features, target_features]):\n        ax = fig.add_subplot(grid_spec[i, j], projection=\"3d\")\n        plot_surface_map(\n            features[i, :], axes=ax, vmax=10, vmin=-10, colorbar=False\n        )\n\n    # Add labels to subplots\n    if i == 0:\n        for j in range(2):\n            ax = fig.add_subplot(grid_spec[i, j])\n            ax.axis(\"off\")\n            ax.text(\n                0.5,\n                1,\n                \"source subject\" if j == 0 else \"target subject\",\n                va=\"center\",\n                ha=\"center\",\n            )\n\n    ax = fig.add_subplot(grid_spec[i, :])\n    ax.axis(\"off\")\n    ax.text(0.5, 0, contrast_name, va=\"center\", ha=\"center\")\n\n# Add colorbar\nax = fig.add_subplot(grid_spec[2, :])\nax.axis(\"off\")\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"2%\")\nfig.add_axes(cax)\nfig.colorbar(\n    mpl.cm.ScalarMappable(\n        norm=mpl.colors.Normalize(vmin=-10, vmax=10), cmap=\"coolwarm\"\n    ),\n    cax=cax,\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating geometry kernel matrices\nThis time, let's assume matrices of size ``(n, n)`` (or ``(n, m)``,\n``(m, m)``) won't fit in memory, even with a powerful GPU -\nas a rule of thumb, this will typically be the case if ``n`` is greater\nthan 50k. Therefore, the source and target geometry matrices cannot be\nexplicitly computed. Instead, we derive an embedding ``X`` in dimension\n``k`` to approximate these matrices. Under the hood, this embedding\napproximates the geodesic distance between all pairs of vertices by\ncomputing the true geodesic distance to ``n_landmarks`` vertices\nthat are randomly sampled.\nHigher values of ``n_landmarks`` lead to more precise embeddings, although\nthey will take more time to compute. Note that this does not affect the\nspeed of the rest of the alignment procedure, so you might want to invest\ncomputational time in deriving precise embeddings. Moreover, this step\ncan easily be parallelized on CPUs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(coordinates, triangles) = surface.load_surf_mesh(fsaverage5.pial_left)\nfs5_pial_left_geometry_embeddings = lmds.compute_lmds(\n    coordinates,\n    triangles,\n    n_landmarks=100,\n    k=3,\n    n_jobs=2,\n    verbose=True,\n)\nsource_geometry_embeddings = fs5_pial_left_geometry_embeddings\ntarget_geometry_embeddings = fs5_pial_left_geometry_embeddings\nsource_geometry_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each line ``vertex_index`` of the geometry matrices contains the anatomical\ndistance (here in millimeters) from ``vertex_index`` to all other vertices\nof the mesh.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vertex_index = 12\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"Approximated geodesic distance in mm\\non the cortical surface\")\nplot_surface_map(\n    np.linalg.norm(\n        source_geometry_embeddings\n        - source_geometry_embeddings[vertex_index, :],\n        axis=1,\n    ),\n    cmap=\"magma\",\n    cbar_tick_format=\"%.2f\",\n    axes=ax,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing features and geometries\nFeatures and embeddings should be normalized before we can train a mapping.\nNormalizing embeddings can be tricky, so we provide an empirical method\nto perform this operation: ``coarse_to_fine.random_normalizing()``\nsamples pairs of indices in the embeddings vector and computes their\nL2 norm. Eventually, it divides the embeddings vector by the maximum\nnorm it found in this process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_features_normalized = source_features / np.linalg.norm(\n    source_features, axis=1\n).reshape(-1, 1)\ntarget_features_normalized = target_features / np.linalg.norm(\n    target_features, axis=1\n).reshape(-1, 1)\n\nsource_embeddings_normalized, source_distance_max = (\n    coarse_to_fine.random_normalizing(source_geometry_embeddings)\n)\ntarget_embeddings_normalized, target_distance_max = (\n    coarse_to_fine.random_normalizing(target_geometry_embeddings)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the mapping\nLet's create our mappings. We will need 2 of them: one for computing\nan alignment between sub-samples of the source and target individuals,\nthe other one for computing a fine-grained alignment leveraging\ninformation gathered during the coarse step.\nNote that the 2 solvers can use different parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coarse_mapping = FUGW(alpha=0.5, rho=1, eps=1e-4)\ncoarse_mapping_solver = \"mm\"\ncoarse_mapping_solver_params = {\n    \"nits_bcd\": 5,\n    \"tol_uot\": 1e-10,\n}\n\nfine_mapping = FUGWSparse(alpha=0.5, rho=1, eps=1e-4)\nfine_mapping_solver = \"mm\"\nfine_mapping_solver_params = {\n    \"nits_bcd\": 3,\n    \"tol_uot\": 1e-10,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before fitting our mappings, we sub-sample vertices from the source and\ntarget distributions. We could sample them randomly, but a poor sampling\nof some cortical areas can have really bad consequences\non the fine-grained mapping.\nTherefore, we try to sample vertices uniformly spread across the cortical\nsurface. We use a util function which leverages the ward algorithm to\ncompute this sampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_sample = coarse_to_fine.sample_mesh_uniformly(\n    coordinates,\n    triangles,\n    embeddings=source_geometry_embeddings,\n    n_samples=1000,\n)\ntarget_sample = coarse_to_fine.sample_mesh_uniformly(\n    coordinates,\n    triangles,\n    embeddings=target_geometry_embeddings,\n    n_samples=1000,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect the sampled vertices used to derive the coarse mapping.\nWe here switch to ``plotly`` to generate interactive figures.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_sampled_surface = np.zeros(source_features.shape[1])\nsource_sampled_surface[source_sample] = 1\ntarget_sampled_surface = np.zeros(target_features.shape[1])\ntarget_sampled_surface[target_sample] = 1\n\n# Generate figure with 2 subplots\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    specs=[[{\"type\": \"surface\"}, {\"type\": \"surface\"}]],\n    subplot_titles=[\"Source subject\", \"Target subject\"],\n)\n\nfig_source = plot_surface_map(\n    source_sampled_surface, cmap=\"Blues\", engine=\"plotly\"\n)\nfig_target = plot_surface_map(\n    target_sampled_surface, cmap=\"Blues\", engine=\"plotly\"\n)\nfig.add_trace(fig_source.figure.data[0], row=1, col=1)\nfig.add_trace(fig_target.figure.data[0], row=1, col=2)\n\n# Edit figure layout\nlayout = copy.deepcopy(LAYOUT)\nm = 50\nlayout[\"margin\"] = {\"l\": m, \"r\": m, \"t\": m, \"b\": m, \"pad\": 0}\nlayout[\"scene\"][\"camera\"] = CAMERAS[\"left\"]\nlayout[\"scene\"][\"camera\"][\"eye\"] = {\"x\": -1.5, \"y\": 0, \"z\": 0}\nlayout[\"scene2\"] = layout[\"scene\"]\n\nfig.update_layout(\n    **layout, height=400, width=800, title_text=\"Sampled vertices\", title_x=0.5\n)\n\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rationale behind sampling uniformly across the cortical surface\nis that only vertices which are within a certain radius of sub-sampled\nvertices will eventually appear in the sparsity mask, and therefore\nin the transport plan.\nIn other words, vertices which are too far from sub-sampled vertices\nwon't be mapped.\nLet us set a radius of 7 millimeters to see what proportion of vertices\nwill be allowed to be transported.\nWe leverage the properties of sparse matrices to\nderive a scalable way to derive which vertices are selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_selection_radius = 7\nn_neighbourhoods_per_vertex_source = (\n    torch.sparse.sum(\n        coarse_to_fine.get_neighbourhood_matrix(\n            source_geometry_embeddings, source_sample, source_selection_radius\n        ),\n        dim=1,\n    )\n    .to_dense()\n    .numpy()\n)\n\ntarget_selection_radius = 7\nn_neighbourhoods_per_vertex_target = (\n    torch.sparse.sum(\n        coarse_to_fine.get_neighbourhood_matrix(\n            target_geometry_embeddings, target_sample, target_selection_radius\n        ),\n        dim=1,\n    )\n    .to_dense()\n    .numpy()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now check which vertices will appear in the sparsity mask.\nIn the following figure, deep blue is for vertices which were\nsampled, light blue for vertices which are within radius-distance\nof a sampled vertex. Vertices which won't be selected appear in white.\nThe following figure is interactive.\n**Note that, because embeddings are not very precise for short distances,\nvertices that are very close to sampled vertices can actually\nbe absent from the mask**. In order to limit this effect, the radius\nshould generally be set to a high enough value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_vertices_in_mask = np.zeros(source_features.shape[1])\nsource_vertices_in_mask[n_neighbourhoods_per_vertex_source > 0] = 1\nsource_vertices_in_mask[source_sample] = 2\ntarget_vertices_in_mask = np.zeros(target_features.shape[1])\ntarget_vertices_in_mask[n_neighbourhoods_per_vertex_target > 0] = 1\ntarget_vertices_in_mask[target_sample] = 2\n\n# Generate figure with 2 subplots\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    specs=[[{\"type\": \"surface\"}, {\"type\": \"surface\"}]],\n    subplot_titles=[\"Source subject\", \"Target subject\"],\n)\n\nfig_source = plot_surface_map(\n    source_vertices_in_mask, cmap=\"Blues\", engine=\"plotly\"\n)\nfig_target = plot_surface_map(\n    target_vertices_in_mask, cmap=\"Blues\", engine=\"plotly\"\n)\n\nfig.add_trace(fig_source.figure.data[0], row=1, col=1)\nfig.add_trace(fig_target.figure.data[0], row=1, col=2)\n\n# Edit figure layout\nlayout = copy.deepcopy(LAYOUT)\nm = 50\nlayout[\"margin\"] = {\"l\": m, \"r\": m, \"t\": m, \"b\": m, \"pad\": 0}\nlayout[\"scene\"][\"camera\"] = CAMERAS[\"left\"]\nlayout[\"scene\"][\"camera\"][\"eye\"] = {\"x\": -1.5, \"y\": 0, \"z\": 0}\nlayout[\"scene2\"] = layout[\"scene\"]\n\nfig.update_layout(\n    **layout,\n    height=400,\n    width=800,\n    title_text=\"Sampled & and selected vertices\",\n    title_x=0.5,\n)\n\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just saw that, given the number of vertices sampled on the\nsource and target subjects, a 7mm selection radius will not cover\nthe entier cortical surface.\nOne could increase the radius to circumvent this issue, but this\ncomes at a high computational cost. We generally advise\nto increase the number of sampled vertices.\nFor practicality, we increase the radius to 10mm in this example.\n\nNow, let's fit our mappings! \ud83d\ude80\n\nRemember to use the training maps only. **Note that the radius\nshould be normalized by the same coefficient that was used\nto normalize the respective embeddings matrices**.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n\ncoarse_to_fine.fit(\n    # Source and target's features and embeddings\n    source_features=source_features_normalized[:n_training_contrasts, :],\n    target_features=target_features_normalized[:n_training_contrasts, :],\n    source_geometry_embeddings=source_embeddings_normalized,\n    target_geometry_embeddings=target_embeddings_normalized,\n    # Parametrize step 1 (coarse alignment between source and target)\n    source_sample=source_sample,\n    target_sample=target_sample,\n    coarse_mapping=coarse_mapping,\n    coarse_mapping_solver=coarse_mapping_solver,\n    coarse_mapping_solver_params=coarse_mapping_solver_params,\n    # Parametrize step 2 (selection of pairs of indices present in\n    # fine-grained's sparsity mask)\n    coarse_pairs_selection_method=\"topk\",\n    source_selection_radius=10 / source_distance_max,\n    target_selection_radius=10 / target_distance_max,\n    # Parametrize step 3 (fine-grained alignment)\n    fine_mapping=fine_mapping,\n    fine_mapping_solver=fine_mapping_solver,\n    fine_mapping_solver_params=fine_mapping_solver_params,\n    # Misc\n    verbose=True,\n)\n\nt1 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the evolution of the FUGW loss while traning of the coarse mapping,\nwith and without the entropic term. As you can see, we most likely\nstopped fitting the coarse mapping too early, yet it is probably enough\nfor this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4, 4))\nax.set_title(\"Coarse mapping's training loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"BCD step\")\nax.plot(coarse_mapping.loss_steps, coarse_mapping.loss, label=\"FUGW loss\")\nax.plot(\n    coarse_mapping.loss_steps,\n    coarse_mapping.loss_entropic,\n    label=\"FUGW entropic loss\",\n)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is that of the fine-grained mapping:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4, 4))\nax.set_title(\"Fine mapping's training loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"BCD step\")\nax.plot(fine_mapping.loss_steps, fine_mapping.loss, label=\"FUGW loss\")\nax.plot(\n    fine_mapping.loss_steps,\n    fine_mapping.loss_entropic,\n    label=\"FUGW entropic loss\",\n)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how few iterations are needed for the fine-grained model to converge\ncompared to the coarse one, although the coarse model usually runs much\nfaster (in total time) than the fine-grained one.\nIt probably is a good strategy to invest computational time in deriving\na precise coarse solution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Total training time: {t1 - t0:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the computed mappings\nIn this example, the transport plan of the coarse mapping\nis already too big to be displayed, but we can still look at the top-left\ncorder.\nThis plot will not always be informative, as mapped vertices are sampled\nat random. In this example, it does show some structure though,\ndue to the fact that the source and target meshes are the same,\nand that our uniform sampling strategy returns compable results\nfor the 2 distributions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coarse_pi = coarse_mapping.pi.numpy()\nfig, ax = plt.subplots(figsize=(10, 10))\nax.set_title(\"Coarse transport plan (top-left corner)\", fontsize=20)\nax.set_xlabel(\"target vertices\", fontsize=15)\nax.set_ylabel(\"source vertices\", fontsize=15)\nim = plt.imshow(coarse_pi[:200, :200], cmap=\"viridis\")\nplt.colorbar(im, ax=ax, shrink=0.8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we can visualize the sparse transport plan computed by the\nfine-grained mapping, which is much more informative.\nIndeed, it exhibits some structure because the source and target meshes\nare the same: indeed, assuming vertex correspondence between the source\nand target mesh should already yield a reasonable alignment,\nwe expected the diagonal of this matrix to be non-null.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "indices = fine_mapping.pi.indices()\nfine_mapping_as_scipy_coo = coo_matrix(\n    (\n        fine_mapping.pi.values(),\n        (indices[0], indices[1]),\n    ),\n    shape=fine_mapping.pi.size(),\n)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.set_title(\"Sparsity mask of fine-grained mapping\", fontsize=15)\nax.set_ylabel(\"Source vertices\", fontsize=15)\nax.set_xlabel(\"Target vertices\", fontsize=15)\nplt.spy(fine_mapping_as_scipy_coo, precision=\"present\", markersize=0.01)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the computed sparse transport plan is quite sparse:\nit stores about 0.5% of what the equivalent dense transport plan\nwould store.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "100 * fine_mapping.pi.values().shape[0] / fine_mapping.pi.shape.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each line ``vertex_index`` of the computed mapping can be interpreted as\na probability map describing which vertices of the target\nshould be mapped with the source vertex ``vertex_index``.\nSince the ith row of a sparse matrix is not always easy to access,\nwe fetch it by using ``.inverse_transform()`` on a one-hot vertor\nwhose only non-null coefficient is at position ``vertex_index``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "one_hot = np.zeros(source_features.shape[1])\none_hot[vertex_index] = 1.0\nprobability_map = fine_mapping.inverse_transform(one_hot)\n\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\n    \"Probability map of target vertices\\n\"\n    f\"being matched with source vertex {vertex_index}\"\n)\nplot_surface_map(probability_map, cmap=\"viridis\", axes=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using ``mapping.transform()``,\nwe can use the computed mapping to transport any collection of feature maps\nfrom the source anatomy onto the target anatomy.\nNote that, conversely, ``mapping.inverse_transform()`` takes feature maps\nfrom the target anatomy and transports them on the source anatomy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_index = 2\npredicted_target_features = fine_mapping.transform(\n    source_features[contrast_index, :]\n)\npredicted_target_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(3 * 3, 3))\nfig.suptitle(\"Transporting feature maps of the training set\")\ngrid_spec = gridspec.GridSpec(1, 3, figure=fig)\n\nax = fig.add_subplot(grid_spec[0, 0], projection=\"3d\")\nax.set_title(\"Actual source features\")\nplot_surface_map(\n    source_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nax = fig.add_subplot(grid_spec[0, 1], projection=\"3d\")\nax.set_title(\"Predicted target features\")\nplot_surface_map(predicted_target_features, axes=ax, vmax=10, vmin=-10)\n\nax = fig.add_subplot(grid_spec[0, 2], projection=\"3d\")\nax.set_title(\"Actual target features\")\nplot_surface_map(\n    target_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we transported a feature map which is part of the traning set,\nwhich does not really help evaluate the quality of our model.\nInstead, we can also use the computed mapping to transport unseen data,\nwhich is how we will usually assess whether our model has captured\nuseful information or not:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_index = len(contrasts) - 1\npredicted_target_features = fine_mapping.transform(\n    source_features[contrast_index, :]\n)\n\nfig = plt.figure(figsize=(3 * 3, 3))\nfig.suptitle(\"Transporting feature maps of the test set\")\ngrid_spec = gridspec.GridSpec(1, 3, figure=fig)\n\nax = fig.add_subplot(grid_spec[0, 0], projection=\"3d\")\nax.set_title(\"Actual source features\")\nplot_surface_map(\n    source_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nax = fig.add_subplot(grid_spec[0, 1], projection=\"3d\")\nax.set_title(\"Predicted target features\")\nplot_surface_map(predicted_target_features, axes=ax, vmax=10, vmin=-10)\n\nax = fig.add_subplot(grid_spec[0, 2], projection=\"3d\")\nax.set_title(\"Actual target features\")\nplot_surface_map(\n    target_features[contrast_index, :], axes=ax, vmax=10, vmin=-10\n)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}